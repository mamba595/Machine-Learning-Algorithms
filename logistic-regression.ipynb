{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:36.123747Z","iopub.execute_input":"2025-07-27T13:32:36.124051Z","iopub.status.idle":"2025-07-27T13:32:38.614169Z","shell.execute_reply.started":"2025-07-27T13:32:36.124026Z","shell.execute_reply":"2025-07-27T13:32:38.613308Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# read the data\ntitanic = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n# get a glimpse over the data\ntitanic.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:38.615714Z","iopub.execute_input":"2025-07-27T13:32:38.616339Z","iopub.status.idle":"2025-07-27T13:32:38.673881Z","shell.execute_reply.started":"2025-07-27T13:32:38.616312Z","shell.execute_reply":"2025-07-27T13:32:38.672663Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# reviewing the data, I find out that all the numerical columns \n# are complete, except for the Age column, which is normal, \n# since some people may have sneaked into the boat to immigrate to America\ntitanic.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:38.674904Z","iopub.execute_input":"2025-07-27T13:32:38.675348Z","iopub.status.idle":"2025-07-27T13:32:38.713968Z","shell.execute_reply.started":"2025-07-27T13:32:38.675309Z","shell.execute_reply":"2025-07-27T13:32:38.712976Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       PassengerId    Survived      Pclass         Age       SibSp  \\\ncount   891.000000  891.000000  891.000000  714.000000  891.000000   \nmean    446.000000    0.383838    2.308642   29.699118    0.523008   \nstd     257.353842    0.486592    0.836071   14.526497    1.102743   \nmin       1.000000    0.000000    1.000000    0.420000    0.000000   \n25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n75%     668.500000    1.000000    3.000000   38.000000    1.000000   \nmax     891.000000    1.000000    3.000000   80.000000    8.000000   \n\n            Parch        Fare  \ncount  891.000000  891.000000  \nmean     0.381594   32.204208  \nstd      0.806057   49.693429  \nmin      0.000000    0.000000  \n25%      0.000000    7.910400  \n50%      0.000000   14.454200  \n75%      0.000000   31.000000  \nmax      6.000000  512.329200  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>714.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>446.000000</td>\n      <td>0.383838</td>\n      <td>2.308642</td>\n      <td>29.699118</td>\n      <td>0.523008</td>\n      <td>0.381594</td>\n      <td>32.204208</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>257.353842</td>\n      <td>0.486592</td>\n      <td>0.836071</td>\n      <td>14.526497</td>\n      <td>1.102743</td>\n      <td>0.806057</td>\n      <td>49.693429</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>223.500000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>20.125000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.910400</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>446.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.454200</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>668.500000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>31.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>891.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>80.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>512.329200</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# since Cabin was an object, it didn't appear in the describe method,\n# but now we know it has a lot of null values\ntitanic.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:38.716070Z","iopub.execute_input":"2025-07-27T13:32:38.716417Z","iopub.status.idle":"2025-07-27T13:32:38.742059Z","shell.execute_reply.started":"2025-07-27T13:32:38.716385Z","shell.execute_reply":"2025-07-27T13:32:38.741156Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# the Cabin column has 147 different unique values,\n# which makes one hot encoding inefficient\ntitanic['Cabin'].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:38.742804Z","iopub.execute_input":"2025-07-27T13:32:38.743049Z","iopub.status.idle":"2025-07-27T13:32:38.765302Z","shell.execute_reply.started":"2025-07-27T13:32:38.743027Z","shell.execute_reply":"2025-07-27T13:32:38.764189Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"147"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# the Ticket column just tells you the \n# type of ticket, as \"STON/02.\" may show.\n# ordinal encoding may be useful here\ntitanic['Ticket']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:38.766414Z","iopub.execute_input":"2025-07-27T13:32:38.766996Z","iopub.status.idle":"2025-07-27T13:32:38.792464Z","shell.execute_reply.started":"2025-07-27T13:32:38.766959Z","shell.execute_reply":"2025-07-27T13:32:38.791013Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0             A/5 21171\n1              PC 17599\n2      STON/O2. 3101282\n3                113803\n4                373450\n             ...       \n886              211536\n887              112053\n888          W./C. 6607\n889              111369\n890              370376\nName: Ticket, Length: 891, dtype: object"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# in the data description of the competition,\n# it says there are 3 ports, \n# C = Cherbourg\n# Q = Queenstown\n# S = Southampton\n# this is perfect for one hot encoding\ntitanic['Embarked']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:38.793673Z","iopub.execute_input":"2025-07-27T13:32:38.794326Z","iopub.status.idle":"2025-07-27T13:32:38.815157Z","shell.execute_reply.started":"2025-07-27T13:32:38.794292Z","shell.execute_reply":"2025-07-27T13:32:38.813823Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0      S\n1      C\n2      S\n3      S\n4      S\n      ..\n886    S\n887    S\n888    S\n889    C\n890    Q\nName: Embarked, Length: 891, dtype: object"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# I will drop this column, since it's useless\ntitanic['PassengerId']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:38.816495Z","iopub.execute_input":"2025-07-27T13:32:38.816901Z","iopub.status.idle":"2025-07-27T13:32:38.840737Z","shell.execute_reply.started":"2025-07-27T13:32:38.816868Z","shell.execute_reply":"2025-07-27T13:32:38.839734Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0        1\n1        2\n2        3\n3        4\n4        5\n      ... \n886    887\n887    888\n888    889\n889    890\n890    891\nName: PassengerId, Length: 891, dtype: int64"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# I will create a new feature for the amount of family members\n# family members = siblings + spouses + parents + children\ntitanic['FamMembers'] = titanic['SibSp'] + titanic['Parch']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:38.841830Z","iopub.execute_input":"2025-07-27T13:32:38.842094Z","iopub.status.idle":"2025-07-27T13:32:38.857592Z","shell.execute_reply.started":"2025-07-27T13:32:38.842067Z","shell.execute_reply":"2025-07-27T13:32:38.856547Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# splitting the data into training and validation sets\nfrom sklearn.model_selection import train_test_split\n\ny = titanic['Survived']\nX = titanic.drop('Survived', axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:38.861475Z","iopub.execute_input":"2025-07-27T13:32:38.864982Z","iopub.status.idle":"2025-07-27T13:32:40.460650Z","shell.execute_reply.started":"2025-07-27T13:32:38.864941Z","shell.execute_reply":"2025-07-27T13:32:40.459421Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# I drop PassengerId column\nX_train.drop('PassengerId', axis=1, inplace=True)\nX_valid.drop('PassengerId', axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:40.461806Z","iopub.execute_input":"2025-07-27T13:32:40.462883Z","iopub.status.idle":"2025-07-27T13:32:40.470340Z","shell.execute_reply.started":"2025-07-27T13:32:40.462849Z","shell.execute_reply":"2025-07-27T13:32:40.469125Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# separating the data between numerical and categorical columns.\n# since the dataset doesn't have too many columns, I will keep all\nnumerical_cols = X_train.select_dtypes(include=['int64','float64']).columns\ncategorical_cols = X_train.select_dtypes(include=['object']).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:40.471917Z","iopub.execute_input":"2025-07-27T13:32:40.472268Z","iopub.status.idle":"2025-07-27T13:32:40.490177Z","shell.execute_reply.started":"2025-07-27T13:32:40.472238Z","shell.execute_reply":"2025-07-27T13:32:40.489222Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# we obtain One Hot Encoding, Ordinal Encoding and Target Encoding columns\nohe_cols = [col for col in categorical_cols if X_train[col].nunique() <= 3]\nordinal_cols = [col for col in categorical_cols if 3 < X_train[col].nunique() < 10]\nhigh_cardinality_cols = [col for col in categorical_cols if X_train[col].nunique() >= 10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:40.491264Z","iopub.execute_input":"2025-07-27T13:32:40.491572Z","iopub.status.idle":"2025-07-27T13:32:40.513141Z","shell.execute_reply.started":"2025-07-27T13:32:40.491544Z","shell.execute_reply":"2025-07-27T13:32:40.511659Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# fill null values with the median in numerical columns\nfrom sklearn.impute import SimpleImputer\n\nnum_cols_transformer = SimpleImputer(strategy='median')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:40.514625Z","iopub.execute_input":"2025-07-27T13:32:40.515012Z","iopub.status.idle":"2025-07-27T13:32:40.815013Z","shell.execute_reply.started":"2025-07-27T13:32:40.514986Z","shell.execute_reply":"2025-07-27T13:32:40.814027Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# pipelines for categorical columns\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom category_encoders import TargetEncoder\nfrom sklearn.pipeline import Pipeline\n\nohe_cols_transformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('ohe', OneHotEncoder(handle_unknown='ignore',\n                           sparse_output=False))\n])\n\nordinal_cols_transformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value',\n                                unknown_value=-1))\n])\n\nhigh_cardinality_cols_transformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('target', TargetEncoder())\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:40.815995Z","iopub.execute_input":"2025-07-27T13:32:40.816256Z","iopub.status.idle":"2025-07-27T13:32:41.629161Z","shell.execute_reply.started":"2025-07-27T13:32:40.816230Z","shell.execute_reply":"2025-07-27T13:32:41.628162Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# bundle all the pipelines into a ColumnTransformer\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_cols_transformer, numerical_cols),\n        ('ohe', ohe_cols_transformer, ohe_cols),\n        ('ord', ordinal_cols_transformer, ordinal_cols),\n        ('hcc', high_cardinality_cols_transformer, high_cardinality_cols)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:41.630082Z","iopub.execute_input":"2025-07-27T13:32:41.630532Z","iopub.status.idle":"2025-07-27T13:32:41.651703Z","shell.execute_reply.started":"2025-07-27T13:32:41.630501Z","shell.execute_reply":"2025-07-27T13:32:41.650823Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# prepare the data for pytorch\nimport torch\nfrom sklearn.preprocessing import StandardScaler\n\nX_train = preprocessor.fit_transform(X_train, y_train)\nX_valid = preprocessor.transform(X_valid)\n\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train)\nX_valid_scaled = scaler_X.transform(X_valid)\n\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\nX_valid_tensor = torch.tensor(X_valid_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\ny_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:32:41.652552Z","iopub.execute_input":"2025-07-27T13:32:41.652987Z","iopub.status.idle":"2025-07-27T13:32:47.063845Z","shell.execute_reply.started":"2025-07-27T13:32:41.652962Z","shell.execute_reply":"2025-07-27T13:32:47.062775Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# logistic regression model using SGD\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\n\ntrain_ds = TensorDataset(X_train_tensor, y_train_tensor)\nbs = 8 # batch size\ntrain_dl = DataLoader(train_ds, bs, shuffle=True)\n\n# define the neural network\nmodel = nn.Sequential(\n    nn.Linear(X_train.shape[1], 1),\n    nn.Sigmoid()\n)\n\n# define optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n\n# define loss function as Binary Cross-Entropy Loss\nloss_fn = nn.BCELoss()\n\n# the training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    for x, y in train_dl:\n        # produce the predictions\n        predictions = model(x).squeeze()\n        \n        # obtain the loss\n        loss = loss_fn(predictions, y)\n\n        # compute the gradients\n        loss.backward()\n\n        # clip the values so that the gradients dont explode\n        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n\n        # update the model parameters\n        optimizer.step()\n\n        # clear the gradients\n        optimizer.zero_grad()\n\n    with torch.no_grad():\n        predictions = model(X_train_tensor).squeeze()\n        epoch_loss = loss_fn(predictions, y_train_tensor)\n        print(f'Epoch: {epoch} Loss: {epoch_loss.item()}')\n\nloss = loss_fn(model(X_valid_tensor).squeeze(), y_valid_tensor)\nprint(loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:40:46.318440Z","iopub.execute_input":"2025-07-27T13:40:46.318840Z","iopub.status.idle":"2025-07-27T13:41:23.477668Z","shell.execute_reply.started":"2025-07-27T13:40:46.318811Z","shell.execute_reply":"2025-07-27T13:41:23.476553Z"}},"outputs":[{"name":"stdout","text":"Epoch: 0 Loss: 0.7122110724449158\nEpoch: 1 Loss: 0.6617990732192993\nEpoch: 2 Loss: 0.6153187155723572\nEpoch: 3 Loss: 0.5738932490348816\nEpoch: 4 Loss: 0.5363302230834961\nEpoch: 5 Loss: 0.5028159618377686\nEpoch: 6 Loss: 0.4726055860519409\nEpoch: 7 Loss: 0.4454127550125122\nEpoch: 8 Loss: 0.42095792293548584\nEpoch: 9 Loss: 0.3987632989883423\nEpoch: 10 Loss: 0.3787113130092621\nEpoch: 11 Loss: 0.3604706823825836\nEpoch: 12 Loss: 0.3438483774662018\nEpoch: 13 Loss: 0.3286518454551697\nEpoch: 14 Loss: 0.314712792634964\nEpoch: 15 Loss: 0.3018898367881775\nEpoch: 16 Loss: 0.2900587022304535\nEpoch: 17 Loss: 0.27911677956581116\nEpoch: 18 Loss: 0.26897624135017395\nEpoch: 19 Loss: 0.25954434275627136\nEpoch: 20 Loss: 0.25075361132621765\nEpoch: 21 Loss: 0.24254514276981354\nEpoch: 22 Loss: 0.23486192524433136\nEpoch: 23 Loss: 0.22765477001667023\nEpoch: 24 Loss: 0.22088457643985748\nEpoch: 25 Loss: 0.21451182663440704\nEpoch: 26 Loss: 0.20850302278995514\nEpoch: 27 Loss: 0.20282907783985138\nEpoch: 28 Loss: 0.1974618285894394\nEpoch: 29 Loss: 0.19237788021564484\nEpoch: 30 Loss: 0.18755443394184113\nEpoch: 31 Loss: 0.18297235667705536\nEpoch: 32 Loss: 0.17861394584178925\nEpoch: 33 Loss: 0.1744634509086609\nEpoch: 34 Loss: 0.17050683498382568\nEpoch: 35 Loss: 0.16672970354557037\nEpoch: 36 Loss: 0.16312040388584137\nEpoch: 37 Loss: 0.15966849029064178\nEpoch: 38 Loss: 0.156363844871521\nEpoch: 39 Loss: 0.15319696068763733\nEpoch: 40 Loss: 0.15015889704227448\nEpoch: 41 Loss: 0.14724256098270416\nEpoch: 42 Loss: 0.1444406807422638\nEpoch: 43 Loss: 0.14174611866474152\nEpoch: 44 Loss: 0.13915292918682098\nEpoch: 45 Loss: 0.1366560161113739\nEpoch: 46 Loss: 0.1342497020959854\nEpoch: 47 Loss: 0.1319291740655899\nEpoch: 48 Loss: 0.12968961894512177\nEpoch: 49 Loss: 0.12752710282802582\nEpoch: 50 Loss: 0.1254376620054245\nEpoch: 51 Loss: 0.1234174370765686\nEpoch: 52 Loss: 0.12146320939064026\nEpoch: 53 Loss: 0.1195717304944992\nEpoch: 54 Loss: 0.11774024367332458\nEpoch: 55 Loss: 0.1159655824303627\nEpoch: 56 Loss: 0.11424506455659866\nEpoch: 57 Loss: 0.11257646232843399\nEpoch: 58 Loss: 0.11095740646123886\nEpoch: 59 Loss: 0.10938557982444763\nEpoch: 60 Loss: 0.10785900801420212\nEpoch: 61 Loss: 0.1063757985830307\nEpoch: 62 Loss: 0.104933962225914\nEpoch: 63 Loss: 0.10353200137615204\nEpoch: 64 Loss: 0.10216812789440155\nEpoch: 65 Loss: 0.10084076970815659\nEpoch: 66 Loss: 0.09954867511987686\nEpoch: 67 Loss: 0.09829015284776688\nEpoch: 68 Loss: 0.09706401079893112\nEpoch: 69 Loss: 0.09586906433105469\nEpoch: 70 Loss: 0.09470409154891968\nEpoch: 71 Loss: 0.09356796741485596\nEpoch: 72 Loss: 0.09245958924293518\nEpoch: 73 Loss: 0.09137804806232452\nEpoch: 74 Loss: 0.09032223373651505\nEpoch: 75 Loss: 0.08929137885570526\nEpoch: 76 Loss: 0.088284432888031\nEpoch: 77 Loss: 0.08730069547891617\nEpoch: 78 Loss: 0.08633929491043091\nEpoch: 79 Loss: 0.08539944887161255\nEpoch: 80 Loss: 0.08448058366775513\nEpoch: 81 Loss: 0.08358193933963776\nEpoch: 82 Loss: 0.08270285278558731\nEpoch: 83 Loss: 0.0818425789475441\nEpoch: 84 Loss: 0.08100060373544693\nEpoch: 85 Loss: 0.08017639815807343\nEpoch: 86 Loss: 0.07936928421258926\nEpoch: 87 Loss: 0.07857874035835266\nEpoch: 88 Loss: 0.07780434936285019\nEpoch: 89 Loss: 0.07704555988311768\nEpoch: 90 Loss: 0.07630186527967453\nEpoch: 91 Loss: 0.07557287812232971\nEpoch: 92 Loss: 0.074858158826828\nEpoch: 93 Loss: 0.07415725290775299\nEpoch: 94 Loss: 0.07346978038549423\nEpoch: 95 Loss: 0.07279538363218307\nEpoch: 96 Loss: 0.07213367521762848\nEpoch: 97 Loss: 0.07148423045873642\nEpoch: 98 Loss: 0.07084683328866959\nEpoch: 99 Loss: 0.07022102922201157\nEpoch: 100 Loss: 0.06960649788379669\nEpoch: 101 Loss: 0.06900301575660706\nEpoch: 102 Loss: 0.0684102475643158\nEpoch: 103 Loss: 0.06782790273427963\nEpoch: 104 Loss: 0.06725575774908066\nEpoch: 105 Loss: 0.0666934922337532\nEpoch: 106 Loss: 0.06614083796739578\nEpoch: 107 Loss: 0.06559758633375168\nEpoch: 108 Loss: 0.06506350636482239\nEpoch: 109 Loss: 0.06453834474086761\nEpoch: 110 Loss: 0.06402181833982468\nEpoch: 111 Loss: 0.06351383775472641\nEpoch: 112 Loss: 0.06301409751176834\nEpoch: 113 Loss: 0.06252242624759674\nEpoch: 114 Loss: 0.062038604170084\nEpoch: 115 Loss: 0.061562493443489075\nEpoch: 116 Loss: 0.061093881726264954\nEpoch: 117 Loss: 0.06063258647918701\nEpoch: 118 Loss: 0.06017841026186943\nEpoch: 119 Loss: 0.05973125249147415\nEpoch: 120 Loss: 0.05929085984826088\nEpoch: 121 Loss: 0.05885716900229454\nEpoch: 122 Loss: 0.05842999741435051\nEpoch: 123 Loss: 0.058009207248687744\nEpoch: 124 Loss: 0.05759461969137192\nEpoch: 125 Loss: 0.057186104357242584\nEpoch: 126 Loss: 0.0567835308611393\nEpoch: 127 Loss: 0.05638681724667549\nEpoch: 128 Loss: 0.055995821952819824\nEpoch: 129 Loss: 0.05561038851737976\nEpoch: 130 Loss: 0.05523039773106575\nEpoch: 131 Loss: 0.05485572665929794\nEpoch: 132 Loss: 0.05448630824685097\nEpoch: 133 Loss: 0.054122019559144974\nEpoch: 134 Loss: 0.053762730211019516\nEpoch: 135 Loss: 0.05340837687253952\nEpoch: 136 Loss: 0.05305878445506096\nEpoch: 137 Loss: 0.052713871002197266\nEpoch: 138 Loss: 0.05237359553575516\nEpoch: 139 Loss: 0.052037812769412994\nEpoch: 140 Loss: 0.051706474274396896\nEpoch: 141 Loss: 0.05137947201728821\nEpoch: 142 Loss: 0.05105675384402275\nEpoch: 143 Loss: 0.05073823779821396\nEpoch: 144 Loss: 0.05042378231883049\nEpoch: 145 Loss: 0.05011333152651787\nEpoch: 146 Loss: 0.04980679973959923\nEpoch: 147 Loss: 0.04950413107872009\nEpoch: 148 Loss: 0.04920526593923569\nEpoch: 149 Loss: 0.048910096287727356\nEpoch: 150 Loss: 0.04861858859658241\nEpoch: 151 Loss: 0.0483306460082531\nEpoch: 152 Loss: 0.048046208918094635\nEpoch: 153 Loss: 0.047765202820301056\nEpoch: 154 Loss: 0.04748759791254997\nEpoch: 155 Loss: 0.04721330106258392\nEpoch: 156 Loss: 0.046942293643951416\nEpoch: 157 Loss: 0.0466744489967823\nEpoch: 158 Loss: 0.04640977829694748\nEpoch: 159 Loss: 0.04614821448922157\nEpoch: 160 Loss: 0.045889660716056824\nEpoch: 161 Loss: 0.04563413932919502\nEpoch: 162 Loss: 0.04538150131702423\nEpoch: 163 Loss: 0.045131754130125046\nEpoch: 164 Loss: 0.0448848120868206\nEpoch: 165 Loss: 0.0446406789124012\nEpoch: 166 Loss: 0.04439929500222206\nEpoch: 167 Loss: 0.044160570949316025\nEpoch: 168 Loss: 0.043924491852521896\nEpoch: 169 Loss: 0.043691013008356094\nEpoch: 170 Loss: 0.04346011206507683\nEpoch: 171 Loss: 0.04323169216513634\nEpoch: 172 Loss: 0.04300575703382492\nEpoch: 173 Loss: 0.042782265692949295\nEpoch: 174 Loss: 0.042561184614896774\nEpoch: 175 Loss: 0.042342446744441986\nEpoch: 176 Loss: 0.04212605208158493\nEpoch: 177 Loss: 0.04191192239522934\nEpoch: 178 Loss: 0.04170002415776253\nEpoch: 179 Loss: 0.041490353643894196\nEpoch: 180 Loss: 0.041282862424850464\nEpoch: 181 Loss: 0.04107748344540596\nEpoch: 182 Loss: 0.040874190628528595\nEpoch: 183 Loss: 0.04067303612828255\nEpoch: 184 Loss: 0.04047389328479767\nEpoch: 185 Loss: 0.040276769548654556\nEpoch: 186 Loss: 0.04008160158991814\nEpoch: 187 Loss: 0.039888396859169006\nEpoch: 188 Loss: 0.03969713672995567\nEpoch: 189 Loss: 0.03950772434473038\nEpoch: 190 Loss: 0.039320215582847595\nEpoch: 191 Loss: 0.03913453221321106\nEpoch: 192 Loss: 0.038950636982917786\nEpoch: 193 Loss: 0.03876851499080658\nEpoch: 194 Loss: 0.03858817741274834\nEpoch: 195 Loss: 0.03840957209467888\nEpoch: 196 Loss: 0.038232654333114624\nEpoch: 197 Loss: 0.03805743530392647\nEpoch: 198 Loss: 0.037883859127759933\nEpoch: 199 Loss: 0.03771193325519562\nEpoch: 200 Loss: 0.03754158318042755\nEpoch: 201 Loss: 0.03737286105751991\nEpoch: 202 Loss: 0.03720569983124733\nEpoch: 203 Loss: 0.03704005479812622\nEpoch: 204 Loss: 0.03687593713402748\nEpoch: 205 Loss: 0.03671332076191902\nEpoch: 206 Loss: 0.03655219078063965\nEpoch: 207 Loss: 0.03639254719018936\nEpoch: 208 Loss: 0.036234300583601\nEpoch: 209 Loss: 0.03607747703790665\nEpoch: 210 Loss: 0.03592207282781601\nEpoch: 211 Loss: 0.0357680469751358\nEpoch: 212 Loss: 0.035615380853414536\nEpoch: 213 Loss: 0.03546405956149101\nEpoch: 214 Loss: 0.03531408682465553\nEpoch: 215 Loss: 0.03516540303826332\nEpoch: 216 Loss: 0.035018038004636765\nEpoch: 217 Loss: 0.03487194702029228\nEpoch: 218 Loss: 0.034727081656455994\nEpoch: 219 Loss: 0.03458348289132118\nEpoch: 220 Loss: 0.034441109746694565\nEpoch: 221 Loss: 0.03429991379380226\nEpoch: 222 Loss: 0.03415995463728905\nEpoch: 223 Loss: 0.034021150320768356\nEpoch: 224 Loss: 0.033883534371852875\nEpoch: 225 Loss: 0.03374705836176872\nEpoch: 226 Loss: 0.033611737191677094\nEpoch: 227 Loss: 0.03347751125693321\nEpoch: 228 Loss: 0.03334439918398857\nEpoch: 229 Loss: 0.03321237862110138\nEpoch: 230 Loss: 0.03308143466711044\nEpoch: 231 Loss: 0.03295154869556427\nEpoch: 232 Loss: 0.032822709530591965\nEpoch: 233 Loss: 0.03269492834806442\nEpoch: 234 Loss: 0.03256818279623985\nEpoch: 235 Loss: 0.03244243189692497\nEpoch: 236 Loss: 0.03231770545244217\nEpoch: 237 Loss: 0.03219398856163025\nEpoch: 238 Loss: 0.03207122161984444\nEpoch: 239 Loss: 0.03194942697882652\nEpoch: 240 Loss: 0.031828563660383224\nEpoch: 241 Loss: 0.031708668917417526\nEpoch: 242 Loss: 0.03158968314528465\nEpoch: 243 Loss: 0.03147166594862938\nEpoch: 244 Loss: 0.03135454282164574\nEpoch: 245 Loss: 0.031238285824656487\nEpoch: 246 Loss: 0.031122952699661255\nEpoch: 247 Loss: 0.031008511781692505\nEpoch: 248 Loss: 0.030894925817847252\nEpoch: 249 Loss: 0.030782198533415794\nEpoch: 250 Loss: 0.030670316889882088\nEpoch: 251 Loss: 0.03055928833782673\nEpoch: 252 Loss: 0.03044907934963703\nEpoch: 253 Loss: 0.030339695513248444\nEpoch: 254 Loss: 0.03023112565279007\nEpoch: 255 Loss: 0.03012336790561676\nEpoch: 256 Loss: 0.030016368255019188\nEpoch: 257 Loss: 0.029910197481513023\nEpoch: 258 Loss: 0.02980477549135685\nEpoch: 259 Loss: 0.0297001451253891\nEpoch: 260 Loss: 0.029596267268061638\nEpoch: 261 Loss: 0.029493143782019615\nEpoch: 262 Loss: 0.02939075604081154\nEpoch: 263 Loss: 0.029289109632372856\nEpoch: 264 Loss: 0.02918815053999424\nEpoch: 265 Loss: 0.029087936505675316\nEpoch: 266 Loss: 0.028988441452383995\nEpoch: 267 Loss: 0.028889629989862442\nEpoch: 268 Loss: 0.0287915151566267\nEpoch: 269 Loss: 0.028694091364741325\nEpoch: 270 Loss: 0.02859736606478691\nEpoch: 271 Loss: 0.028501294553279877\nEpoch: 272 Loss: 0.028405921533703804\nEpoch: 273 Loss: 0.028311191126704216\nEpoch: 274 Loss: 0.028217103332281113\nEpoch: 275 Loss: 0.02812367118895054\nEpoch: 276 Loss: 0.028030915185809135\nEpoch: 277 Loss: 0.02793876826763153\nEpoch: 278 Loss: 0.02784724161028862\nEpoch: 279 Loss: 0.027756310999393463\nEpoch: 280 Loss: 0.027665982022881508\nEpoch: 281 Loss: 0.02757628820836544\nEpoch: 282 Loss: 0.02748718671500683\nEpoch: 283 Loss: 0.027398720383644104\nEpoch: 284 Loss: 0.027310853824019432\nEpoch: 285 Loss: 0.027223527431488037\nEpoch: 286 Loss: 0.027136843651533127\nEpoch: 287 Loss: 0.02705063670873642\nEpoch: 288 Loss: 0.026965055614709854\nEpoch: 289 Loss: 0.02688000164926052\nEpoch: 290 Loss: 0.026795491576194763\nEpoch: 291 Loss: 0.02671155519783497\nEpoch: 292 Loss: 0.026628145948052406\nEpoch: 293 Loss: 0.026545263826847076\nEpoch: 294 Loss: 0.026462947949767113\nEpoch: 295 Loss: 0.026381149888038635\nEpoch: 296 Loss: 0.026299862191081047\nEpoch: 297 Loss: 0.026219123974442482\nEpoch: 298 Loss: 0.02613889053463936\nEpoch: 299 Loss: 0.026059173047542572\nEpoch: 300 Loss: 0.025979960337281227\nEpoch: 301 Loss: 0.02590120956301689\nEpoch: 302 Loss: 0.025822971016168594\nEpoch: 303 Loss: 0.025745239108800888\nEpoch: 304 Loss: 0.02566799335181713\nEpoch: 305 Loss: 0.02559124492108822\nEpoch: 306 Loss: 0.02551492489874363\nEpoch: 307 Loss: 0.025439072400331497\nEpoch: 308 Loss: 0.025363709777593613\nEpoch: 309 Loss: 0.02528877556324005\nEpoch: 310 Loss: 0.025214318186044693\nEpoch: 311 Loss: 0.02514031156897545\nEpoch: 312 Loss: 0.025066791102290154\nEpoch: 313 Loss: 0.024993708357214928\nEpoch: 314 Loss: 0.02492103911936283\nEpoch: 315 Loss: 0.024848811328411102\nEpoch: 316 Loss: 0.024777034297585487\nEpoch: 317 Loss: 0.024705683812499046\nEpoch: 318 Loss: 0.02463473565876484\nEpoch: 319 Loss: 0.024564240127801895\nEpoch: 320 Loss: 0.02449415996670723\nEpoch: 321 Loss: 0.02442445605993271\nEpoch: 322 Loss: 0.024355165660381317\nEpoch: 323 Loss: 0.024286318570375443\nEpoch: 324 Loss: 0.024217845872044563\nEpoch: 325 Loss: 0.024149786680936813\nEpoch: 326 Loss: 0.02408214844763279\nEpoch: 327 Loss: 0.02401486411690712\nEpoch: 328 Loss: 0.023948002606630325\nEpoch: 329 Loss: 0.02388150244951248\nEpoch: 330 Loss: 0.023815348744392395\nEpoch: 331 Loss: 0.023749614134430885\nEpoch: 332 Loss: 0.023684225976467133\nEpoch: 333 Loss: 0.023619234561920166\nEpoch: 334 Loss: 0.023554624989628792\nEpoch: 335 Loss: 0.02349035069346428\nEpoch: 336 Loss: 0.02342647686600685\nEpoch: 337 Loss: 0.023362943902611732\nEpoch: 338 Loss: 0.02329977974295616\nEpoch: 339 Loss: 0.0232369527220726\nEpoch: 340 Loss: 0.023174457252025604\nEpoch: 341 Loss: 0.023112334311008453\nEpoch: 342 Loss: 0.02305055968463421\nEpoch: 343 Loss: 0.022989120334386826\nEpoch: 344 Loss: 0.022928006947040558\nEpoch: 345 Loss: 0.022867247462272644\nEpoch: 346 Loss: 0.022806795313954353\nEpoch: 347 Loss: 0.022746702656149864\nEpoch: 348 Loss: 0.022686950862407684\nEpoch: 349 Loss: 0.022627517580986023\nEpoch: 350 Loss: 0.02256837859749794\nEpoch: 351 Loss: 0.02250955067574978\nEpoch: 352 Loss: 0.02245105430483818\nEpoch: 353 Loss: 0.02239287458360195\nEpoch: 354 Loss: 0.022335009649395943\nEpoch: 355 Loss: 0.022277452051639557\nEpoch: 356 Loss: 0.02222020924091339\nEpoch: 357 Loss: 0.022163255140185356\nEpoch: 358 Loss: 0.022106612101197243\nEpoch: 359 Loss: 0.02205025963485241\nEpoch: 360 Loss: 0.021994201466441154\nEpoch: 361 Loss: 0.021938450634479523\nEpoch: 362 Loss: 0.02188301831483841\nEpoch: 363 Loss: 0.021827859804034233\nEpoch: 364 Loss: 0.021772954612970352\nEpoch: 365 Loss: 0.02171836420893669\nEpoch: 366 Loss: 0.02166404016315937\nEpoch: 367 Loss: 0.02161002904176712\nEpoch: 368 Loss: 0.021556273102760315\nEpoch: 369 Loss: 0.021502787247300148\nEpoch: 370 Loss: 0.021449586376547813\nEpoch: 371 Loss: 0.021396642550826073\nEpoch: 372 Loss: 0.021344000473618507\nEpoch: 373 Loss: 0.021291609853506088\nEpoch: 374 Loss: 0.021239494904875755\nEpoch: 375 Loss: 0.02118763141334057\nEpoch: 376 Loss: 0.021136021241545677\nEpoch: 377 Loss: 0.021084707230329514\nEpoch: 378 Loss: 0.02103363908827305\nEpoch: 379 Loss: 0.020982831716537476\nEpoch: 380 Loss: 0.020932266488671303\nEpoch: 381 Loss: 0.02088196948170662\nEpoch: 382 Loss: 0.020831892266869545\nEpoch: 383 Loss: 0.020782090723514557\nEpoch: 384 Loss: 0.020732520148158073\nEpoch: 385 Loss: 0.020683204755187035\nEpoch: 386 Loss: 0.02063414826989174\nEpoch: 387 Loss: 0.020585305988788605\nEpoch: 388 Loss: 0.020536713302135468\nEpoch: 389 Loss: 0.020488347858190536\nEpoch: 390 Loss: 0.020440248772501945\nEpoch: 391 Loss: 0.02039235830307007\nEpoch: 392 Loss: 0.02034468948841095\nEpoch: 393 Loss: 0.02029728703200817\nEpoch: 394 Loss: 0.020250089466571808\nEpoch: 395 Loss: 0.020203115418553352\nEpoch: 396 Loss: 0.0201563760638237\nEpoch: 397 Loss: 0.020109841600060463\nEpoch: 398 Loss: 0.020063553005456924\nEpoch: 399 Loss: 0.0200174730271101\nEpoch: 400 Loss: 0.01997162215411663\nEpoch: 401 Loss: 0.01992594450712204\nEpoch: 402 Loss: 0.01988050900399685\nEpoch: 403 Loss: 0.019835302606225014\nEpoch: 404 Loss: 0.019790304824709892\nEpoch: 405 Loss: 0.019745510071516037\nEpoch: 406 Loss: 0.01970093883574009\nEpoch: 407 Loss: 0.019656555727124214\nEpoch: 408 Loss: 0.0196123868227005\nEpoch: 409 Loss: 0.019568420946598053\nEpoch: 410 Loss: 0.019524656236171722\nEpoch: 411 Loss: 0.0194811150431633\nEpoch: 412 Loss: 0.019437767565250397\nEpoch: 413 Loss: 0.01939460076391697\nEpoch: 414 Loss: 0.01935167796909809\nEpoch: 415 Loss: 0.019308920949697495\nEpoch: 416 Loss: 0.019266370683908463\nEpoch: 417 Loss: 0.019224004819989204\nEpoch: 418 Loss: 0.019181815907359123\nEpoch: 419 Loss: 0.01913982629776001\nEpoch: 420 Loss: 0.01909804716706276\nEpoch: 421 Loss: 0.01905645988881588\nEpoch: 422 Loss: 0.019015049561858177\nEpoch: 423 Loss: 0.018973831087350845\nEpoch: 424 Loss: 0.01893278770148754\nEpoch: 425 Loss: 0.018891915678977966\nEpoch: 426 Loss: 0.018851254135370255\nEpoch: 427 Loss: 0.01881079003214836\nEpoch: 428 Loss: 0.018770454451441765\nEpoch: 429 Loss: 0.018730293959379196\nEpoch: 430 Loss: 0.018690353259444237\nEpoch: 431 Loss: 0.018650567159056664\nEpoch: 432 Loss: 0.018610971048474312\nEpoch: 433 Loss: 0.01857154630124569\nEpoch: 434 Loss: 0.0185322817414999\nEpoch: 435 Loss: 0.018493210896849632\nEpoch: 436 Loss: 0.018454287201166153\nEpoch: 437 Loss: 0.018415551632642746\nEpoch: 438 Loss: 0.01837697997689247\nEpoch: 439 Loss: 0.018338562920689583\nEpoch: 440 Loss: 0.018300313502550125\nEpoch: 441 Loss: 0.018262218683958054\nEpoch: 442 Loss: 0.018224313855171204\nEpoch: 443 Loss: 0.018186550587415695\nEpoch: 444 Loss: 0.018148982897400856\nEpoch: 445 Loss: 0.018111547455191612\nEpoch: 446 Loss: 0.018074285238981247\nEpoch: 447 Loss: 0.01803717203438282\nEpoch: 448 Loss: 0.018000219017267227\nEpoch: 449 Loss: 0.017963431775569916\nEpoch: 450 Loss: 0.0179267767816782\nEpoch: 451 Loss: 0.017890291288495064\nEpoch: 452 Loss: 0.017853958532214165\nEpoch: 453 Loss: 0.01781780645251274\nEpoch: 454 Loss: 0.017781786620616913\nEpoch: 455 Loss: 0.01774589903652668\nEpoch: 456 Loss: 0.01771019771695137\nEpoch: 457 Loss: 0.017674624919891357\nEpoch: 458 Loss: 0.017639178782701492\nEpoch: 459 Loss: 0.01760389655828476\nEpoch: 460 Loss: 0.017568768933415413\nEpoch: 461 Loss: 0.017533788457512856\nEpoch: 462 Loss: 0.01749894581735134\nEpoch: 463 Loss: 0.017464222386479378\nEpoch: 464 Loss: 0.017429664731025696\nEpoch: 465 Loss: 0.01739524118602276\nEpoch: 466 Loss: 0.017360961064696312\nEpoch: 467 Loss: 0.017326829954981804\nEpoch: 468 Loss: 0.017292827367782593\nEpoch: 469 Loss: 0.017258981242775917\nEpoch: 470 Loss: 0.017225246876478195\nEpoch: 471 Loss: 0.017191655933856964\nEpoch: 472 Loss: 0.017158176749944687\nEpoch: 473 Loss: 0.01712486520409584\nEpoch: 474 Loss: 0.01709168776869774\nEpoch: 475 Loss: 0.017058636993169785\nEpoch: 476 Loss: 0.017025694251060486\nEpoch: 477 Loss: 0.01699289120733738\nEpoch: 478 Loss: 0.016960222274065018\nEpoch: 479 Loss: 0.016927706077694893\nEpoch: 480 Loss: 0.01689530536532402\nEpoch: 481 Loss: 0.016863038763403893\nEpoch: 482 Loss: 0.01683090254664421\nEpoch: 483 Loss: 0.016798894852399826\nEpoch: 484 Loss: 0.016767002642154694\nEpoch: 485 Loss: 0.016735222190618515\nEpoch: 486 Loss: 0.01670358143746853\nEpoch: 487 Loss: 0.016672084107995033\nEpoch: 488 Loss: 0.016640689224004745\nEpoch: 489 Loss: 0.016609424725174904\nEpoch: 490 Loss: 0.01657828874886036\nEpoch: 491 Loss: 0.01654726453125477\nEpoch: 492 Loss: 0.016516370698809624\nEpoch: 493 Loss: 0.016485588625073433\nEpoch: 494 Loss: 0.01645493134856224\nEpoch: 495 Loss: 0.016424372792243958\nEpoch: 496 Loss: 0.016393952071666718\nEpoch: 497 Loss: 0.016363635659217834\nEpoch: 498 Loss: 0.0163334459066391\nEpoch: 499 Loss: 0.016303353011608124\n0.4563218355178833\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# get the test predictions\ntitanic_test = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ntitanic_test['FamMembers'] = titanic_test['SibSp'] + titanic_test['Parch']\nX_test = titanic_test.drop('PassengerId', axis=1)\nX_test = preprocessor.transform(X_test)\nX_test_scaled = scaler_X.transform(X_test)\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n\nmodel.eval()\nwith torch.no_grad():\n    probabilities = model(X_test_tensor).squeeze()\n    final_predictions = (probabilities > 0.5).int()\n\nsubmission = pd.DataFrame({\n    'PassengerId': titanic_test['PassengerId'],\n    'Survived': final_predictions.numpy()\n})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T13:41:30.573373Z","iopub.execute_input":"2025-07-27T13:41:30.573914Z","iopub.status.idle":"2025-07-27T13:41:30.607136Z","shell.execute_reply.started":"2025-07-27T13:41:30.573861Z","shell.execute_reply":"2025-07-27T13:41:30.606173Z"}},"outputs":[],"execution_count":31}]}