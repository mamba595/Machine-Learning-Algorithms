{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read the data\ntitanic = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n# get a glimpse over the data\ntitanic.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reviewing the data, I find out that all the numerical columns \n# are complete, except for the Age column, which is normal, \n# since some people may have sneaked into the boat to immigrate to America\ntitanic.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# since Cabin was an object, it didn't appear in the describe method,\n# but now we know it has a lot of null values\ntitanic.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the Cabin column has 147 different unique values,\n# which makes one hot encoding inefficient\ntitanic['Cabin'].nunique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the Ticket column just tells you the \n# type of ticket, as \"STON/02.\" may show.\n# ordinal encoding may be useful here\ntitanic['Ticket']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# in the data description of the competition,\n# it says there are 3 ports, \n# C = Cherbourg\n# Q = Queenstown\n# S = Southampton\n# this is perfect for one hot encoding\ntitanic['Embarked']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I will drop this column, since it's useless\ntitanic['PassengerId']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I will create a new feature for the amount of family members\n# family members = siblings + spouses + parents + children\ntitanic['FamMembers'] = titanic['SibSp'] + titanic['Parch']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# splitting the data into training and validation sets\nfrom sklearn.model_selection import train_test_split\n\ny = titanic['Survived']\nX = titanic.drop('Survived', axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I drop PassengerId column\nX_train.drop('PassengerId', axis=1, inplace=True)\nX_valid.drop('PassengerId', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# separating the data between numerical and categorical columns.\n# since the dataset doesn't have too many columns, I will keep all\nnumerical_cols = X_train.select_dtypes(include=['int64','float64']).columns\ncategorical_cols = X_train.select_dtypes(include=['object']).columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# we obtain One Hot Encoding, Ordinal Encoding and Target Encoding columns\nohe_cols = [col for col in categorical_cols if X_train[col].nunique() <= 3]\nordinal_cols = [col for col in categorical_cols if 3 < X_train[col].nunique() < 10]\nhigh_cardinality_cols = [col for col in categorical_cols if X_train[col].nunique() >= 10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fill null values with the median in numerical columns\nfrom sklearn.impute import SimpleImputer\n\nnum_cols_transformer = SimpleImputer(strategy='median')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pipelines for categorical columns\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom category_encoders import TargetEncoder\nfrom sklearn.pipeline import Pipeline\n\nohe_cols_transformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('ohe', OneHotEncoder(handle_unknown='ignore',\n                           sparse_output=False))\n])\n\nordinal_cols_transformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value',\n                                unknown_value=-1))\n])\n\nhigh_cardinality_cols_transformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('target', TargetEncoder())\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# bundle all the pipelines into a ColumnTransformer\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_cols_transformer, numerical_cols),\n        ('ohe', ohe_cols_transformer, ohe_cols),\n        ('ord', ordinal_cols_transformer, ordinal_cols),\n        ('hcc', high_cardinality_cols_transformer, high_cardinality_cols)\n    ]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prepare the data for pytorch\nimport torch\nfrom sklearn.preprocessing import StandardScaler\n\nX_train = preprocessor.fit_transform(X_train, y_train)\nX_valid = preprocessor.transform(X_valid)\n\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train)\nX_valid_scaled = scaler_X.transform(X_valid)\n\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\nX_valid_tensor = torch.tensor(X_valid_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\ny_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# logistic regression model using SGD\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\n\ntrain_ds = TensorDataset(X_train_tensor, y_train_tensor)\nbs = 8 # batch size\ntrain_dl = DataLoader(train_ds, bs, shuffle=True)\n\n# define the neural network\nmodel = nn.Sequential(\n    nn.Linear(X_train.shape[1], 1),\n    nn.Sigmoid()\n)\n\n# define optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n\n# define loss function as Binary Cross-Entropy Loss\nloss_fn = nn.BCELoss()\n\n# the training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    for x, y in train_dl:\n        # produce the predictions\n        predictions = model(x).squeeze()\n        \n        # obtain the loss\n        loss = loss_fn(predictions, y)\n\n        # compute the gradients\n        loss.backward()\n\n        # clip the values so that the gradients dont explode\n        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n\n        # update the model parameters\n        optimizer.step()\n\n        # clear the gradients\n        optimizer.zero_grad()\n\n    with torch.no_grad():\n        predictions = model(X_train_tensor).squeeze()\n        epoch_loss = loss_fn(predictions, y_train_tensor)\n        print(f'Epoch: {epoch} Loss: {epoch_loss.item()}')\n\nloss = loss_fn(model(X_valid_tensor).squeeze(), y_valid_tensor)\nprint(loss.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get the test predictions\ntitanic_test = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ntitanic_test['FamMembers'] = titanic_test['SibSp'] + titanic_test['Parch']\nX_test = titanic_test.drop('PassengerId', axis=1)\nX_test = preprocessor.transform(X_test)\nX_test_scaled = scaler_X.transform(X_test)\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n\nmodel.eval()\nwith torch.no_grad():\n    probabilities = model(X_test_tensor).squeeze()\n    final_predictions = (probabilities > 0.5).int()\n\nsubmission = pd.DataFrame({\n    'PassengerId': titanic_test['PassengerId'],\n    'Survived': final_predictions.numpy()\n})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}