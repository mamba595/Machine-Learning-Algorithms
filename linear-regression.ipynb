{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:28:59.605752Z","iopub.execute_input":"2025-07-25T18:28:59.606049Z","iopub.status.idle":"2025-07-25T18:29:02.008350Z","shell.execute_reply.started":"2025-07-25T18:28:59.606024Z","shell.execute_reply":"2025-07-25T18:29:02.007572Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"DATA PREPROCESSING","metadata":{}},{"cell_type":"code","source":"# Numerical columns must be separated from categorical ones\n# Categorical ones can be one hot encoded or ordinal encoded\n# First, delete columns with >80% null values if its correlation to target variable is <10%\n# Second, null values in numerical columns must be replaced with the median value using SimpleImputer median strategy\n# Third, null values in categorical columns must be replaced with the most common value, adding another column to say if the value was missing\n# Fourth, non null entries in categorical columns must be one hot encoded if unique values are <= 3\n# Otherwise, non entries in categorical columns must be ordinal encoded if a ranking exists\n# Else, apply frequency encoding and normalize the values\n# Numerical values must be normalized from 0 to 1, by taking each value and dividing it to the max value or MinMaxScaler\n# Put all of this in a sklearn pipeline\n# Finally, all the columns must be concatenated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:02.009636Z","iopub.execute_input":"2025-07-25T18:29:02.010092Z","iopub.status.idle":"2025-07-25T18:29:02.014780Z","shell.execute_reply.started":"2025-07-25T18:29:02.010063Z","shell.execute_reply":"2025-07-25T18:29:02.013914Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# read the data\nhousing = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\nhousing.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:02.015778Z","iopub.execute_input":"2025-07-25T18:29:02.016086Z","iopub.status.idle":"2025-07-25T18:29:02.132441Z","shell.execute_reply.started":"2025-07-25T18:29:02.016059Z","shell.execute_reply":"2025-07-25T18:29:02.131578Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1460 entries, 0 to 1459\nData columns (total 81 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Id             1460 non-null   int64  \n 1   MSSubClass     1460 non-null   int64  \n 2   MSZoning       1460 non-null   object \n 3   LotFrontage    1201 non-null   float64\n 4   LotArea        1460 non-null   int64  \n 5   Street         1460 non-null   object \n 6   Alley          91 non-null     object \n 7   LotShape       1460 non-null   object \n 8   LandContour    1460 non-null   object \n 9   Utilities      1460 non-null   object \n 10  LotConfig      1460 non-null   object \n 11  LandSlope      1460 non-null   object \n 12  Neighborhood   1460 non-null   object \n 13  Condition1     1460 non-null   object \n 14  Condition2     1460 non-null   object \n 15  BldgType       1460 non-null   object \n 16  HouseStyle     1460 non-null   object \n 17  OverallQual    1460 non-null   int64  \n 18  OverallCond    1460 non-null   int64  \n 19  YearBuilt      1460 non-null   int64  \n 20  YearRemodAdd   1460 non-null   int64  \n 21  RoofStyle      1460 non-null   object \n 22  RoofMatl       1460 non-null   object \n 23  Exterior1st    1460 non-null   object \n 24  Exterior2nd    1460 non-null   object \n 25  MasVnrType     588 non-null    object \n 26  MasVnrArea     1452 non-null   float64\n 27  ExterQual      1460 non-null   object \n 28  ExterCond      1460 non-null   object \n 29  Foundation     1460 non-null   object \n 30  BsmtQual       1423 non-null   object \n 31  BsmtCond       1423 non-null   object \n 32  BsmtExposure   1422 non-null   object \n 33  BsmtFinType1   1423 non-null   object \n 34  BsmtFinSF1     1460 non-null   int64  \n 35  BsmtFinType2   1422 non-null   object \n 36  BsmtFinSF2     1460 non-null   int64  \n 37  BsmtUnfSF      1460 non-null   int64  \n 38  TotalBsmtSF    1460 non-null   int64  \n 39  Heating        1460 non-null   object \n 40  HeatingQC      1460 non-null   object \n 41  CentralAir     1460 non-null   object \n 42  Electrical     1459 non-null   object \n 43  1stFlrSF       1460 non-null   int64  \n 44  2ndFlrSF       1460 non-null   int64  \n 45  LowQualFinSF   1460 non-null   int64  \n 46  GrLivArea      1460 non-null   int64  \n 47  BsmtFullBath   1460 non-null   int64  \n 48  BsmtHalfBath   1460 non-null   int64  \n 49  FullBath       1460 non-null   int64  \n 50  HalfBath       1460 non-null   int64  \n 51  BedroomAbvGr   1460 non-null   int64  \n 52  KitchenAbvGr   1460 non-null   int64  \n 53  KitchenQual    1460 non-null   object \n 54  TotRmsAbvGrd   1460 non-null   int64  \n 55  Functional     1460 non-null   object \n 56  Fireplaces     1460 non-null   int64  \n 57  FireplaceQu    770 non-null    object \n 58  GarageType     1379 non-null   object \n 59  GarageYrBlt    1379 non-null   float64\n 60  GarageFinish   1379 non-null   object \n 61  GarageCars     1460 non-null   int64  \n 62  GarageArea     1460 non-null   int64  \n 63  GarageQual     1379 non-null   object \n 64  GarageCond     1379 non-null   object \n 65  PavedDrive     1460 non-null   object \n 66  WoodDeckSF     1460 non-null   int64  \n 67  OpenPorchSF    1460 non-null   int64  \n 68  EnclosedPorch  1460 non-null   int64  \n 69  3SsnPorch      1460 non-null   int64  \n 70  ScreenPorch    1460 non-null   int64  \n 71  PoolArea       1460 non-null   int64  \n 72  PoolQC         7 non-null      object \n 73  Fence          281 non-null    object \n 74  MiscFeature    54 non-null     object \n 75  MiscVal        1460 non-null   int64  \n 76  MoSold         1460 non-null   int64  \n 77  YrSold         1460 non-null   int64  \n 78  SaleType       1460 non-null   object \n 79  SaleCondition  1460 non-null   object \n 80  SalePrice      1460 non-null   int64  \ndtypes: float64(3), int64(35), object(43)\nmemory usage: 924.0+ KB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# separate the target from the predictors\ny = housing.SalePrice\nX = housing.drop([\"SalePrice\"], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:02.134373Z","iopub.execute_input":"2025-07-25T18:29:02.134642Z","iopub.status.idle":"2025-07-25T18:29:02.144616Z","shell.execute_reply.started":"2025-07-25T18:29:02.134620Z","shell.execute_reply":"2025-07-25T18:29:02.143559Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# divide data into training and validation sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:02.145610Z","iopub.execute_input":"2025-07-25T18:29:02.145925Z","iopub.status.idle":"2025-07-25T18:29:03.825607Z","shell.execute_reply.started":"2025-07-25T18:29:02.145900Z","shell.execute_reply":"2025-07-25T18:29:03.824717Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# delete the id column, which provides no useful information and may pollute the model\nX_train.drop('Id', axis=1, inplace=True)\nX_valid.drop('Id', axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:03.826467Z","iopub.execute_input":"2025-07-25T18:29:03.826935Z","iopub.status.idle":"2025-07-25T18:29:03.834995Z","shell.execute_reply.started":"2025-07-25T18:29:03.826903Z","shell.execute_reply":"2025-07-25T18:29:03.834130Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# filter out the columns with >70% of missing values\nX_train = X_train[ [col for col in X_train.columns if X_train[col].notnull().sum() > 0.3 * X_train.shape[0]]]\nX_valid = X_valid[X_train.columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:03.835888Z","iopub.execute_input":"2025-07-25T18:29:03.836199Z","iopub.status.idle":"2025-07-25T18:29:03.878302Z","shell.execute_reply.started":"2025-07-25T18:29:03.836169Z","shell.execute_reply":"2025-07-25T18:29:03.877581Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# separate between numerical and categorical columns\nnumerical_cols = X_train.select_dtypes(include=[\"int64\",\"float64\"]).columns\ncategorical_cols = X_train.select_dtypes(include=[\"object\"]).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:03.879063Z","iopub.execute_input":"2025-07-25T18:29:03.879378Z","iopub.status.idle":"2025-07-25T18:29:03.896808Z","shell.execute_reply.started":"2025-07-25T18:29:03.879347Z","shell.execute_reply":"2025-07-25T18:29:03.895921Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# separate between high and low cardinality categorical columns\nlow_cardinality_cols = [col for col in categorical_cols if X_train[col].nunique() <= 3]\nhigh_cardinality_cols = [col for col in categorical_cols if 4 <= X_train[col].nunique() < 10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:03.897920Z","iopub.execute_input":"2025-07-25T18:29:03.898247Z","iopub.status.idle":"2025-07-25T18:29:03.923387Z","shell.execute_reply.started":"2025-07-25T18:29:03.898218Z","shell.execute_reply":"2025-07-25T18:29:03.922471Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# drop columns that dont fit the criteria\nfinal_columns = list(numerical_cols) + low_cardinality_cols + high_cardinality_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:03.926428Z","iopub.execute_input":"2025-07-25T18:29:03.926737Z","iopub.status.idle":"2025-07-25T18:29:03.936647Z","shell.execute_reply.started":"2025-07-25T18:29:03.926702Z","shell.execute_reply":"2025-07-25T18:29:03.935662Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"X_train = X_train[final_columns]\nX_valid = X_valid[final_columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:03.937668Z","iopub.execute_input":"2025-07-25T18:29:03.938366Z","iopub.status.idle":"2025-07-25T18:29:03.956432Z","shell.execute_reply.started":"2025-07-25T18:29:03.938334Z","shell.execute_reply":"2025-07-25T18:29:03.955498Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# pipeline for numerical columns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', MinMaxScaler())\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:03.957615Z","iopub.execute_input":"2025-07-25T18:29:03.958313Z","iopub.status.idle":"2025-07-25T18:29:04.238208Z","shell.execute_reply.started":"2025-07-25T18:29:03.958281Z","shell.execute_reply":"2025-07-25T18:29:04.237336Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# pipelines for categorical columns\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nlow_cardinality_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore',\n                             sparse_output=False))\n]) \n\nhigh_cardinality_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder(\n        handle_unknown='use_encoded_value',\n        unknown_value=-1\n    ))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:04.239054Z","iopub.execute_input":"2025-07-25T18:29:04.239407Z","iopub.status.idle":"2025-07-25T18:29:04.245900Z","shell.execute_reply.started":"2025-07-25T18:29:04.239375Z","shell.execute_reply":"2025-07-25T18:29:04.244819Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# bundle preprocessing\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('ohe', low_cardinality_transformer, low_cardinality_cols),\n        ('ord', high_cardinality_transformer, high_cardinality_cols)\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:04.247117Z","iopub.execute_input":"2025-07-25T18:29:04.247969Z","iopub.status.idle":"2025-07-25T18:29:04.280969Z","shell.execute_reply.started":"2025-07-25T18:29:04.247939Z","shell.execute_reply":"2025-07-25T18:29:04.280121Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# prepare the data for pytorch\nimport torch\nfrom sklearn.preprocessing import StandardScaler\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train)\nX_valid_scaled = scaler_X.transform(X_valid)\n\nscaler_Y = StandardScaler()\ny_train_scaled = scaler_Y.fit_transform(y_train.values.reshape(-1,1)).squeeze()\ny_valid_scaled = scaler_Y.transform(y_valid.values.reshape(-1,1)).squeeze()\n\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\nX_valid_tensor = torch.tensor(X_valid_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\ny_valid_tensor = torch.tensor(y_valid_scaled, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:04.281964Z","iopub.execute_input":"2025-07-25T18:29:04.282265Z","iopub.status.idle":"2025-07-25T18:29:09.829802Z","shell.execute_reply.started":"2025-07-25T18:29:04.282240Z","shell.execute_reply":"2025-07-25T18:29:09.828875Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"y_valid_tensor.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:29:09.830774Z","iopub.execute_input":"2025-07-25T18:29:09.831220Z","iopub.status.idle":"2025-07-25T18:29:09.837446Z","shell.execute_reply.started":"2025-07-25T18:29:09.831196Z","shell.execute_reply":"2025-07-25T18:29:09.836647Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"torch.Size([292])"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# linear regression model using SGD\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\n\n# define the dataset for training\ntrain_ds = TensorDataset(X_train_tensor, y_train_tensor)\n\n# define the data loader\nbs = 8 # batch size\ntrain_dl = DataLoader(train_ds, bs, shuffle=True)\n\n# define model\nmodel = nn.Linear(X_train.shape[1], 1)\n\n# define optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# define loss function as Mean Square Error\nloss_fn = nn.MSELoss()\n\n# the training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for x, y in train_dl:\n        # produce the predictions\n        predictions = model(x).squeeze()\n\n        # clip the values so that the gradients dont explode\n        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n        \n        # obtain the loss\n        loss = loss_fn(predictions, y)\n\n        # compute the gradients\n        loss.backward()\n\n        # update the model parameters\n        optimizer.step()\n\n        # clear the gradients\n        optimizer.zero_grad()\n\n    with torch.no_grad():\n        predictions = model(X_train_tensor).squeeze()\n        epoch_loss = loss_fn(predictions, y_train_tensor)\n        print(f'Epoch: {epoch} Loss: {epoch_loss.item()}')\n\nloss = loss_fn(model(X_valid_tensor).squeeze(), y_valid_tensor)\nprint(loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:34:18.292590Z","iopub.execute_input":"2025-07-25T18:34:18.292938Z","iopub.status.idle":"2025-07-25T18:34:19.192356Z","shell.execute_reply.started":"2025-07-25T18:34:18.292913Z","shell.execute_reply":"2025-07-25T18:34:19.191448Z"}},"outputs":[{"name":"stdout","text":"Epoch: 0 Loss: 0.4475018084049225\nEpoch: 1 Loss: 6.877133369445801\nEpoch: 2 Loss: 158.7008514404297\nEpoch: 3 Loss: 4023.657958984375\nEpoch: 4 Loss: 92044.0703125\nEpoch: 5 Loss: 2175543.25\nEpoch: 6 Loss: 51361424.0\nEpoch: 7 Loss: 1204950400.0\nEpoch: 8 Loss: 28638316544.0\nEpoch: 9 Loss: 677174116352.0\n517952544.0\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# get the predictions on the test dataset\nhousing_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\n# preprocess the test dataset\nX_test = housing_test[final_columns]\nX_test = preprocessor.transform(X_test)\nX_test_scaled = scaler_X.transform(X_test)\n\n# set the model to evaluation mode\nmodel.eval()\n\n# make sure gradients are not computed because this is the final model\nwith torch.no_grad():\n    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n    predictions = model(X_test_tensor).squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:34:22.368984Z","iopub.execute_input":"2025-07-25T18:34:22.369297Z","iopub.status.idle":"2025-07-25T18:34:22.424659Z","shell.execute_reply.started":"2025-07-25T18:34:22.369275Z","shell.execute_reply":"2025-07-25T18:34:22.423942Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# submit the predictions\nsubmission = pd.DataFrame({\n    \"Id\": housing_test[\"Id\"],\n    \"SalePrice\": predictions.numpy()\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:34:25.000018Z","iopub.execute_input":"2025-07-25T18:34:25.000307Z","iopub.status.idle":"2025-07-25T18:34:25.009847Z","shell.execute_reply.started":"2025-07-25T18:34:25.000288Z","shell.execute_reply":"2025-07-25T18:34:25.009030Z"}},"outputs":[],"execution_count":38}]}