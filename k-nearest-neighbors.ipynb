{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read the data\ntitanic = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n# get a glimpse over the data\ntitanic.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reviewing the data, I find out that all the numerical columns \n# are complete, except for the Age column, which is normal, \n# since some people may have sneaked into the boat to immigrate to America\ntitanic.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# since Cabin was an object, it didn't appear in the describe method,\n# but now we know it has a lot of null values\ntitanic.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the Cabin column has 147 different unique values,\n# which makes one hot encoding inefficient\ntitanic['Cabin'].nunique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the Ticket column just tells you the \n# type of ticket, as \"STON/02.\" may show.\n# ordinal encoding may be useful here\ntitanic['Ticket']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# in the data description of the competition,\n# it says there are 3 ports, \n# C = Cherbourg\n# Q = Queenstown\n# S = Southampton\n# this is perfect for one hot encoding\ntitanic['Embarked']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I will drop this column, since it's useless\ntitanic['PassengerId']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I will create a new feature for the amount of family members\n# family members = siblings + spouses + parents + children\ntitanic['FamMembers'] = titanic['SibSp'] + titanic['Parch']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# splitting the data into training and validation sets\nfrom sklearn.model_selection import train_test_split\n\ny_train = titanic['Survived']\nX_train = titanic.drop('Survived', axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I drop PassengerId column\nX_train.drop('PassengerId', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# separating the data between numerical and categorical columns.\n# since the dataset doesn't have too many columns, I will keep all\nnumerical_cols = X_train.select_dtypes(include=['int64','float64']).columns\ncategorical_cols = X_train.select_dtypes(include=['object']).columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# we obtain One Hot Encoding, Ordinal Encoding and Target Encoding columns\nohe_cols = [col for col in categorical_cols if X_train[col].nunique() <= 3]\nordinal_cols = [col for col in categorical_cols if 3 < X_train[col].nunique() < 10]\nhigh_cardinality_cols = [col for col in categorical_cols if X_train[col].nunique() >= 10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fill null values with the median in numerical columns\nfrom sklearn.impute import SimpleImputer\n\nnum_cols_transformer = SimpleImputer(strategy='median')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pipelines for categorical columns\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom category_encoders import TargetEncoder\nfrom sklearn.pipeline import Pipeline\n\nohe_cols_transformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('ohe', OneHotEncoder(handle_unknown='ignore',\n                           sparse_output=False))\n])\n\nordinal_cols_transformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value',\n                                unknown_value=-1))\n])\n\nhigh_cardinality_cols_transformer = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('target', TargetEncoder())\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# bundle all the pipelines into a ColumnTransformer\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_cols_transformer, numerical_cols),\n        ('ohe', ohe_cols_transformer, ohe_cols),\n        ('ord', ordinal_cols_transformer, ordinal_cols),\n        ('hcc', high_cardinality_cols_transformer, high_cardinality_cols)\n    ]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prepare the data for pytorch\nimport torch\nfrom sklearn.preprocessing import StandardScaler\n\nX_train = preprocessor.fit_transform(X_train, y_train)\n\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train)\n\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.long)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get the test data\ntitanic_test = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ntitanic_test['FamMembers'] = titanic_test['SibSp'] + titanic_test['Parch']\nX_test = titanic_test.drop('PassengerId', axis=1)\nX_test = preprocessor.transform(X_test)\nX_test_scaled = scaler_X.transform(X_test)\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# apply K Nearest Neighbors\n\n# get the distances between each test point with all training points\ndistances = torch.cdist(X_test_tensor, X_train_tensor)\n\n# gets the closest points to each test point\n_, k_indices = torch.topk(distances, k=3, largest=False)\n\n# gets the label for each of those points\nk_nearest_labels = y_train_tensor[k_indices]\n\n# performs majority vote for each test point and its neighbors\npredictions = torch.mode(k_nearest_labels, dim=1).values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# submit the predictions\nsubmission = pd.DataFrame({\n    'PassengerId': titanic_test['PassengerId'],\n    'Survived': predictions.numpy()\n})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}