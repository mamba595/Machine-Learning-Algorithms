{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"DATA PREPROCESSING","metadata":{}},{"cell_type":"code","source":"# Numerical columns must be separated from categorical ones\n# Categorical ones can be one hot encoded or ordinal encoded\n# First, delete columns with >80% null values if its correlation to target variable is <10%\n# Second, null values in numerical columns must be replaced with the median value using SimpleImputer median strategy\n# Third, null values in categorical columns must be replaced with the most common value, adding another column to say if the value was missing\n# Fourth, non null entries in categorical columns must be one hot encoded if unique values are <= 3\n# Otherwise, non entries in categorical columns must be ordinal encoded if a ranking exists\n# Else, apply frequency encoding and normalize the values\n# Numerical values must be normalized from 0 to 1, by taking each value and dividing it to the max value or MinMaxScaler\n# Put all of this in a sklearn pipeline\n# Finally, all the columns must be concatenated","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read the data\nhousing = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\nhousing.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# separate the target from the predictors\ny = housing.SalePrice\nX = housing.drop([\"SalePrice\"], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# divide data into training and validation sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# delete the id column, which provides no useful information and may pollute the model\nX_train.drop('Id', axis=1, inplace=True)\nX_valid.drop('Id', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filter out the columns with >70% of missing values\nX_train = X_train[ [col for col in X_train.columns if X_train[col].notnull().sum() > 0.3 * X_train.shape[0]]]\nX_valid = X_valid[X_train.columns]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# separate between numerical and categorical columns\nnumerical_cols = X_train.select_dtypes(include=[\"int64\",\"float64\"]).columns\ncategorical_cols = X_train.select_dtypes(include=[\"object\"]).columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# separate between high and low cardinality categorical columns\nlow_cardinality_cols = [col for col in categorical_cols if X_train[col].nunique() <= 3]\nhigh_cardinality_cols = [col for col in categorical_cols if 4 <= X_train[col].nunique() < 10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# drop columns that dont fit the criteria\nfinal_columns = list(numerical_cols) + low_cardinality_cols + high_cardinality_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = X_train[final_columns]\nX_valid = X_valid[final_columns]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pipeline for numerical columns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', MinMaxScaler())\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pipelines for categorical columns\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nlow_cardinality_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore',\n                             sparse_output=False))\n]) \n\nhigh_cardinality_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder(\n        handle_unknown='use_encoded_value',\n        unknown_value=-1\n    ))\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# bundle preprocessing\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('ohe', low_cardinality_transformer, low_cardinality_cols),\n        ('ord', high_cardinality_transformer, high_cardinality_cols)\n    ])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prepare the data for pytorch\nimport torch\nfrom sklearn.preprocessing import StandardScaler\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train)\nX_valid_scaled = scaler_X.transform(X_valid)\n\nscaler_Y = StandardScaler()\ny_train_scaled = scaler_Y.fit_transform(y_train.values.reshape(-1,1)).squeeze()\ny_valid_scaled = scaler_Y.transform(y_valid.values.reshape(-1,1)).squeeze()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Principal Component Analysis in action\nfrom sklearn.decomposition import PCA\n\n# I keep the components that explain 95% of variance\npca = PCA(n_components=0.95)\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_valid_pca = pca.transform(X_valid_scaled)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\nX_valid_tensor = torch.tensor(X_valid_pca, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\ny_valid_tensor = torch.tensor(y_valid_scaled, dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# linear regression model using SGD\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\n\n# define the dataset for training\ntrain_ds = TensorDataset(X_train_tensor, y_train_tensor)\n\n# define the data loader\nbs = 8 # batch size\ntrain_dl = DataLoader(train_ds, bs, shuffle=True)\n\n# define model\nmodel = nn.Linear(X_train_pca.shape[1], 1)\n\n# define optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# define loss function as Mean Square Error\nloss_fn = nn.MSELoss()\n\n# the training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for x, y in train_dl:\n        # produce the predictions\n        predictions = model(x).squeeze()\n\n        # clip the values so that the gradients dont explode\n        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n        \n        # obtain the loss\n        loss = loss_fn(predictions, y)\n\n        # compute the gradients\n        loss.backward()\n\n        # update the model parameters\n        optimizer.step()\n\n        # clear the gradients\n        optimizer.zero_grad()\n\n    with torch.no_grad():\n        predictions = model(X_train_tensor).squeeze()\n        epoch_loss = loss_fn(predictions, y_train_tensor)\n        print(f'Epoch: {epoch} Loss: {epoch_loss.item()}')\n\nloss = loss_fn(model(X_valid_tensor).squeeze(), y_valid_tensor)\nprint(loss.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get the predictions on the test dataset\nhousing_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\n# preprocess the test dataset\nX_test = housing_test[final_columns]\nX_test = preprocessor.transform(X_test)\nX_test_scaled = scaler_X.transform(X_test)\nX_test_pca = pca.transform(X_test_scaled)\n\n# set the model to evaluation mode\nmodel.eval()\n\n# make sure gradients are not computed because this is the final model\nwith torch.no_grad():\n    X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n    predictions = model(X_test_tensor).squeeze()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# submit the predictions\nsubmission = pd.DataFrame({\n    \"Id\": housing_test[\"Id\"],\n    \"SalePrice\": predictions.numpy()\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}